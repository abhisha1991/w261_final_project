{"cells":[{"cell_type":"markdown","source":["## Global Settings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"277258c7-36ac-4214-8cec-8ac5facf665a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, substring, split, when, lit, max as pyspark_max, countDistinct, count, mean, sum as pyspark_sum, expr, to_utc_timestamp, to_timestamp, concat, length\nfrom pyspark.sql import SQLContext, Window \nfrom pyspark.sql.types import IntegerType, StringType, BooleanType, DateType, DoubleType, TimestampType\nimport pandas as pd\nfrom gcmap import GCMapper, Gradient\nimport matplotlib.pyplot as plt\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom datetime import datetime\nfrom pyspark.sql import functions as f\n\nblob_container = \"w261team07container\" # The name of your container created in https://portal.azure.com\nstorage_account = \"w261team07storage\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261team07\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"w261team07-key\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\"\n\nspark.conf.set(\n  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f70b99e8-9ada-4d5c-96e5-f99ac65e0eb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Load Dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"012b68b8-6254-4b88-b98a-ff796700b3e4"}}},{"cell_type":"code","source":["# Inspect the Mount's Final Project folder\ndisplay(dbutils.fs.ls(\"/mnt/mids-w261/datasets_final_project/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5185367-5ec4-4fe6-a95f-0a6048e35f2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# data = spark.read.parquet(f\"{blob_url}/joined_eda/*\")\n# data = spark.read.parquet(f\"{blob_url}/full_join_2015_v0/*\")\n# data = spark.read.parquet(f\"{blob_url}/full_join_with_aggs_v0/*\")\ndata = spark.read.parquet(f\"{blob_url}/model_features_v6/*\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"517c82f6-220c-423e-9865-97abe887ce08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["n = data.count()\nprint(\"The number of rows are {}\".format(n))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba21b611-72e2-4dd5-a669-d7569679f47a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## On-The-Fly Feature Engineering"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37dd4518-4b90-4146-bf62-b463136a37fa"}}},{"cell_type":"code","source":["display(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8103aa3-9443-40be-8eaa-6a0ccb1c35fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["data.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e490c1-b3e7-449c-8e76-e3bb14fd90da"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# null check\nfrom pyspark.sql.functions import isnan, when, count, col\nif False:\n  display(data.select([(100 * count(when(isnan(c) | col(c).isNull(), c))/n).alias(c) for c in data.columns if c != \"planned_departure_utc\"]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"257644a8-eb08-4130-8c8a-076b58c6763b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Helper Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41cc32e3-984a-4a92-abe0-1fb07fe3bc00"}}},{"cell_type":"code","source":["from pyspark.sql.functions import percent_rank, to_timestamp\nfrom pyspark.sql import Window\nfrom datetime import datetime, timedelta\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\n# write model to storage\ndef write_model_to_storage(list_dic, model_class_path, mod_name =''):\n  if len(list_dic) == 0:\n    raise Exception(\"Cannot insert empty object into storage\")\n    \n  # add timestamp as key so we can differentiate models of the same type by time\n  list_dic_new = []\n  now = datetime.now()\n  for d in list_dic:\n    assert(\"train\" in d.keys())\n    assert(\"test\" in d.keys())\n    d[\"timestamp\"] = now\n    list_dic_new.append(d)\n  \n  schema = StructType([ \\\n    StructField(\"timestamp\", TimestampType(), True), \\\n    StructField(\"train\", StringType(), True), \\\n    StructField(\"test\", StringType(), True), \\\n    StructField(\"val\", StringType(), True)])\n  \n  todf = []\n  for d in list_dic_new:\n    todf.append((d[\"timestamp\"], d[\"train\"], d[\"test\"], None))\n    \n  df = spark.createDataFrame(data = todf, schema = schema)\n  \n  # default model name is based on timestamp - to generate unique name\n  if mod_name == '':\n    mod_name = str(now).replace(' ', '').replace(':', '').replace('-', '').split('.')[0]\n  \n  df.write.mode('overwrite').parquet(f\"{blob_url}/{model_class_path}/{mod_name}\")\n\ndef read_model_from_storage(model_path):\n  return spark.read.parquet(f\"{blob_url}/{model_path}/*\")\n  \ndef get_numeric_features(df):\n  return [t[0] for t in df.dtypes if t[1] == 'int' or t[1] == 'double']\n\ndef get_categorical_features(df):\n  return [t[0] for t in df.dtypes if t[1] == 'string']\n\ndef get_datetime_features(df):\n  return  [t[0] for t in df.dtypes if t[1] == 'timestamp']\n\ndef get_boolean_features(df):\n  return  [t[0] for t in df.dtypes if t[1] == 'boolean']\n\ndef assert_no_other_features_exist(df):\n  numeric_features = get_numeric_features(df)\n  categorical_features = get_categorical_features(df)\n  dt_features = get_datetime_features(df)\n  boolean_features = get_boolean_features(df)\n  other_features = [t[0] for t in df.dtypes if t[0] not in numeric_features + categorical_features + dt_features + boolean_features]\n  assert len(other_features) == 0\n\ndef pretty_print_list(elements):\n  print(\"#########################\")\n  for e in elements:\n    print(e)\n  print(\"#########################\")\n  \ndef get_feature_dtype(df, colName):\n  for t in df.dtypes:\n    if t[0] == colName:\n      return t[1]\n  return None\n  \ndef set_feature_dtype(df, colNames, dtype='string'):\n  for colName in colNames:\n    currentType = get_feature_dtype(df, colName)\n    if currentType == None:      \n      raise Exception(\"Colname is not valid: {}\".format(colName))\n    \n    # preserve existing type\n    if currentType == dtype:\n      continue\n      \n    \n    # implicit conversion from bool/str to int is not allowed, for some reason - this problem only appears with \"dep_is_delayed\"\n    # we get back nulls for each row if we do a straight conversion to int\n    # special case to convert dep_is_delayed to int (needed to be in this form for ML models to work)\n    if (currentType == 'string' and colName == \"dep_is_delayed\") and dtype == 'int':\n      \n      def convert_to_int(value):\n        return 1 if value == \"true\" else 0\n        \n      udf_convert = F.udf(convert_to_int, IntegerType())\n    \n      df = df.withColumn(colName + \"_tmp\", udf_convert(colName))\n      df = df.drop(df.dep_is_delayed)\n      df = df.withColumnRenamed(colName + \"_tmp\", colName)\n    \n    \n    elif dtype == 'string':\n      df = df.withColumn(colName, col(colName).cast(StringType()))\n    elif dtype == 'int':\n      df = df.withColumn(colName, col(colName).cast(IntegerType()))\n    elif dtype == 'double':\n      df = df.withColumn(colName, col(colName).cast(DoubleType()))\n    elif dtype == 'boolean':\n      df = df.withColumn(colName, col(colName).cast(BooleanType()))\n    elif dtype == 'timestamp':\n      df = df.withColumn(colName, to_timestamp(colName))\n    else:\n      raise Exception(\"Unsupported data type\")\n  \n  return df\n\ndef get_df_for_model(df, splits, index, datatype=\"train\"):\n  start_date, end_date = get_dates_from_splits(splits, index, dtype = datatype)\n  if verbose:\n    print(\"In method: get_df_for_model - getting back data for data type '{}'. Start date is: {} and End date is: {}\".format(datatype, start_date, end_date))\n  return get_df(df, start_date, end_date, True)\n  \n# gets df between 2 given dates\ndef get_df(df, start_date, end_date, raise_empty=True):\n  # assumes that we have access to planned_departure_utc \n  all_columns = [t[0] for t in df.dtypes]\n  if \"planned_departure_utc\" not in all_columns:\n    raise Exception(\"We cannot slice the data by time because we are missing planned_departure_utc\")\n  \n  df = df.filter((col('planned_departure_utc') >= start_date) & (col('planned_departure_utc') <= end_date))\n  \n  if df.count() == 0 and raise_empty:\n    raise Exception(\"Found 0 records, raising an error as this is not expected\")\n  \n  if verbose:\n    print(\"In method: get_df - getting back data with Start date: {} and End date: {}. Returning {} results\".format(start_date, end_date, df.count()))\n    \n  return df\n\n# contract format depends on function get_timeseries_train_test_splits\ndef get_dates_from_splits(splits, index, dtype=\"train\"):\n  if index >= len(splits):\n    raise Exception(\"Index out of bounds\")\n    \n  split = splits[index]\n    \n  if dtype == \"train\":\n    # 1st 2 dates are training\n    return (split[0], split[1])\n  if dtype == \"test\":\n    # next pair is test\n    return (split[2], split[3])\n  if dtype == \"val\":\n    # last pair is val\n    return (split[4], split[5])\n  \n  # by default return all\n  return split\n\n# get rolling or non-rolling time series splits of data\ndef get_timeseries_train_test_splits(df, rolling=False, roll_months=3, start_year=2015, start_month=1, end_year=2016, end_month=6, train_test_ratio=2, test_months=1):\n  if start_year < 2015 or start_year > 2019:\n    raise Exception(\"Invalid date range\")\n  \n  if start_month < 1 or start_month > 12:\n    raise Exception(\"Invalid date range\")\n  \n  if end_month < 1 or end_month > 12:\n    raise Exception(\"Invalid date range\")\n    \n  if start_year > end_year:\n    raise Exception(\"Start year cannot be larger than end year\")\n  \n  if train_test_ratio <= 1 or int(train_test_ratio) != train_test_ratio:\n    raise Exception(\"train_test_ratio must be > 1 and must be int\")\n  \n  assert(test_months >=1 and train_test_ratio > 1 and roll_months >=1)\n  \n  # assert that we have values for the year and month\n  assert(data.filter(data.year.isNull()).count() == 0)\n  assert(data.filter(data.month.isNull()).count() == 0)\n  \n  # format months to 2 numbers - needed for date time parsing\n  if start_month <= 9:\n    start_month = \"0\" + str(start_month)\n    \n  if end_month <= 9:\n    end_month = \"0\" + str(end_month)\n  \n  \n  global_start = \"{}-{}-01T00:00:00.000+0000\".format(start_year, start_month)\n  # why 28? consider february\n  global_end = \"{}-{}-28T00:00:00.000+0000\".format(end_year, end_month)\n  \n  global_start = datetime.strptime(global_start, '%Y-%m-%dT%H:%M:%S.%f+0000')\n  global_end = datetime.strptime(global_end, '%Y-%m-%dT%H:%M:%S.%f+0000')\n  \n  # check for sufficient data\n  # train data is ratio x num months used for testing, hence (test_months * train_test_ratio)\n  # validation set and test set have same number of months always, hence (2 * test_months)\n  if (global_end - global_start).days < 30 * ((2 * test_months) + (test_months * train_test_ratio)):\n    raise Exception(\"Insufficient data to train on. Please increase date range\")\n  \n  df = df.filter((col('year') >= start_year) & (col('month') >= start_month))\n  df = df.filter((col('year') <= end_year) & (col('month') <= end_month))\n  \n  # create result object - a list of tuple objects\n  # tuple object is of the form of dates: (train_start, train_end, test_start, test_end, val_start, val_end)\n  result = []\n  \n  # train is between start (T0) and X days after start, say (T1)\n  temp_start_train = global_start\n  temp_end_train = global_start + timedelta(days=(test_months * train_test_ratio * 30))\n\n  while (global_end-temp_end_train).days > 0:\n    # test is between T1 and Y days after T1, say T2\n    temp_start_test = temp_end_train + timedelta(days=1) \n    temp_end_test = temp_start_test + timedelta(days=(test_months * 30))\n\n    # validation is between T2 and Y days after T2, say T3\n    temp_start_val = temp_end_test + timedelta(days=1)\n    temp_end_val = temp_start_val + timedelta(days=(test_months * 30))\n\n    # add these dates to our result\n    result.append((temp_start_train, temp_end_train, temp_start_test, temp_end_test, temp_start_val, temp_end_val))\n\n    # reset new date for ending point for train data and repeat till we reach global end date\n    temp_end_train = temp_end_val\n    \n    # if rolling is enabled, we just roll the train start date by the rolling months\n    # and adjust the end train date as well\n    if rolling:\n      temp_start_train = temp_start_train + timedelta(days=30 * roll_months)\n      temp_end_train = temp_start_train + timedelta(days=(test_months * train_test_ratio * 30))\n  \n  if verbose:\n    print(\"There are {} splits formed based on the date ranges given\".format(len(result)))\n    print(\"Date ranges are: start: {} and end: {} with rolling set to {} and rolling window months set to {} months\".format(global_start, global_end, rolling, roll_months))\n    print(\"Note that the train_test_ratio is {} and test_months is {}, so training data will have {} month(s) size and test/val data will have {} month(s) size\".format(train_test_ratio, test_months, train_test_ratio * test_months, test_months))\n    print(\"Here is a sample split that follows the following format: (train_start, train_end, test_start, test_end, val_start, val_end)\")\n    print(pretty_print_list(result[0]))\n  \n  return result\n\ndef get_best_param_dic_metrics(best_model, displayKeys=False):\n  # https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n  parameter_dict = best_model.stages[-1].extractParamMap()\n  dic = dict()\n  for x, y in parameter_dict.items():\n    dic[x.name] = y\n    if displayKeys:\n      print(\"Parameter available: {}\".format(x.name))\n\n  return dic"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccd0d6b3-f3b9-4c46-bc92-fa121c155d17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Feature Engineering (DataType Transformation, Data Prep, ML Algorithms)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eec15ea5-608f-4f69-b14c-98b7246fe5d8"}}},{"cell_type":"code","source":["def get_std_features(data):\n  # Notes\n  # div_reached_dest is going to be full of nulls, so dropping - we should consider making the default as \"-1\" - so it doesn't make us drop rows (dropna)\n  numeric_features = get_numeric_features(data)\n  categorical_features = get_categorical_features(data)\n  dt_features = get_datetime_features(data)\n  bool_features = get_boolean_features(data)\n  assert_no_other_features_exist(data)\n  # no features are null in our model as we dropped/mean imputed/pre-processed them in the data processing stage\n  # so drop their null indicator variables as they provide no value\n  cols_to_drop = ['index_id', 'origin_utc_offset', 'dest_utc_offset', 'origin_latitude', \n                  'origin_longitude', 'dest_latitude', 'dest_longitude', 'dt', 'planned_dep_time', \n                  'actual_dep_time', 'actual_arr_time', 'div_reached_dest', \n                  'time_at_prediction_utc', 'oa_avg_del2_4hr', 'da_avg_del2_4hr', 'carrier_avg_del2_4hr']\\\n                  + [x for x in dt_features if x != 'planned_departure_utc'] + [x for x in numeric_features + categorical_features if x.endswith('_null')]\n  \n  \n  # there are some special snowflakes we need to handle here\n  # dep_is_delayed, origin_altitude and dest_altitude are strings, they should be numeric\n  # so we remove them from the categorical and add them to numeric\n  numeric_features = numeric_features + ['origin_altitude', 'dest_altitude', 'dep_is_delayed']\n  numeric_features = list(set(numeric_features))\n  \n  try:\n    categorical_features.remove('origin_altitude')\n    categorical_features.remove('dest_altitude')\n    categorical_features.remove('dep_is_delayed')\n  except:\n    # dont error if these were not in categorical features\n    pass\n    \n  # likewise, there are some indicator variables that are numeric (int), but need to be string (categorical)\n  ind_vars = [x for x in numeric_features if x.endswith(\"_null\") or x.endswith(\"_ind\")]\n  for x in ind_vars:\n    try:\n      numeric_features.remove(x)\n    except:\n      # dont error if these were not in numeric\n      pass\n    \n  categorical_features = categorical_features + ind_vars\n  categorical_features = list(set(categorical_features))\n  \n  bool_features = [x for x in bool_features if x not in cols_to_drop]\n  dt_features = [x for x in dt_features if x not in cols_to_drop]\n  categorical_features = [x for x in categorical_features if x not in cols_to_drop]\n  numeric_features = [x for x in numeric_features if x not in cols_to_drop]\n  all_cols = numeric_features + categorical_features + dt_features + bool_features\n  cols_to_consider = [x for x in all_cols if x not in cols_to_drop] \n  \n  if verbose:  \n    print(\"There are {} total columns out of which there are {} columns to consider in the model\".format(len(all_cols), len(cols_to_consider)))\n    print(\"There are {} categorical features\".format(len(categorical_features)))\n    print(\"There are {} numeric features\".format(len(numeric_features)))\n    print(\"There are {} date features\".format(len(dt_features)))\n    print(\"There are {} bool features\".format(len(bool_features)))\n    \n  return all_cols, cols_to_consider, cols_to_drop, numeric_features, categorical_features, dt_features, bool_features\n\ndef add_required_cols(cols):\n  # every model must contain the label and the timestamp var\n  if 'planned_departure_utc' not in cols:\n    cols.append('planned_departure_utc')\n  if 'dep_is_delayed' not in cols:\n    cols.append('dep_is_delayed')\n\n  return list(set(cols))\n  \n\ndef get_std_desired_numeric(df, hypothesis=1, custom_cols_to_drop=[]):\n  all_cols, cols_to_consider, cols_to_drop, numeric_features, categorical_features, dt_features, bool_features = get_std_features(data)\n\n  if hypothesis == 1:\n    # all numeric features in the df\n    desired_numeric = [x for x in numeric_features if x in df.columns]\n  elif hypothesis == 2:\n    # includes mandatory features like origin temp and flight dist (or planned duration) + percentage features\n    desired_numeric = ['pct_delayed_from_origin', 'pct_delayed_to_dest', 'pct_delayed_for_route', 'pct_delayed_from_state', 'pct_delayed_to_state', 'flight_distance', 'origin_tmp_c']\n  elif hypothesis == 3:\n    # includes mandatory features like origin temp and flight dist (or planned duration) + weather features\n    desired_numeric = ['origin_altitude', 'origin_wnd_speed', 'origin_cig_cloud_agl', 'origin_vis_dist', 'origin_tmp_c', 'origin_dew_c', 'origin_slp_p',\n                       'dest_altitude', 'planned_duration']\n  elif hypothesis == 4:\n    # includes mandatory features like origin temp and flight dist (or planned duration) + computed columns\n    desired_numeric = ['flight_distance', 'origin_tmp_c', 'pct_delayed_from_origin', 'mean_delay_from_origin', 'pct_delayed_to_dest', \n                       'mean_delay_to_dest', 'pct_delayed_for_route', 'mean_delay_for_route', 'pct_delayed_from_state', 'mean_delay_from_state', \n                       'pct_delayed_to_state', 'mean_delay_to_state']\n  else:\n    raise Exception(\"Invalid hypothesis number!\")\n\n  # drop any columns that are a no-no in the model\n  desired_numeric = [x for x in desired_numeric if x not in cols_to_drop + custom_cols_to_drop]\n  \n  # we must convert dep_is_delayed to numeric\n  desired_numeric = list(set(desired_numeric + ['dep_is_delayed']))\n  \n  # confirm no duplicates\n  assert(len(desired_numeric) == len(set(desired_numeric)))\n\n  # confirm data actually has these features\n  # also confirm that the desired_numeric is part of the \"registered\" numeric features to choose from\n  all_cols = [t[0] for t in df.dtypes]\n  for dn in desired_numeric:\n    if dn not in all_cols:\n      raise Exception(\"Unknown feature found: {}\".format(dn))\n    if dn not in numeric_features:\n      raise Exception(\"Feature: {} is not a registered numeric feature\".format(dn))\n    \n  # ensure that the desired numeric columns are indeed converted to numeric\n  # for example, this will ensure that dep_is_delayed is converted to int\n  to_convert = get_std_to_convert_numeric(df, desired_numeric)\n  df = set_feature_dtype(df, to_convert, dtype='int')\n\n  return df, list(set(desired_numeric))\n\ndef get_std_desired_categorical(df, hypothesis=1, custom_cols_to_drop=[]):\n  all_cols, cols_to_consider, cols_to_drop, numeric_features, categorical_features, dt_features, bool_features = get_std_features(data)\n\n  if hypothesis == 1:\n    # all categorical features in df\n    desired_categorical = [x for x in categorical_features if x in df.columns]\n  elif hypothesis == 2:\n    # includes mandatory features (time related + origin/dest/dist + carrier + holiday) + computed score (potential for delay)\n    desired_categorical = ['month', 'day_of_month', 'day_of_week', 'dep_hour', 'arr_hour', 'origin_ICAO', 'dest_ICAO', 'carrier', 'distance_group', 'holiday', 'poten_for_del']\n  elif hypothesis == 3:\n    # includes mandatory features (time related + origin/dest/dist + carrier + holiday) + computed score (potential for delay) + weather related \n    desired_categorical = ['month', 'day_of_month', 'day_of_week', 'dep_hour', 'arr_hour', 'origin_ICAO', 'dest_ICAO', 'carrier', 'distance_group', \n                           'holiday', 'poten_for_del', 'canceled', 'origin_cig_cavok', 'origin_wnd_type', 'origin_vis_var', 'origin_city', 'dest_city']\n  elif hypothesis == 4:\n    # includes mandatory features (time related + origin/dest/dist + carrier + holiday) + computed score (potential for delay) + computed indicators\n    desired_categorical = ['month', 'day_of_month', 'day_of_week', 'dep_hour', 'arr_hour', 'origin_ICAO', 'dest_ICAO', 'carrier', 'holiday',\n                           'weather_window_del_ind', 'carrier_window_del_ind', 'security_window_del_ind', 'late_ac_window_del_ind', 'nas_window_del_ind',\n                           'oa_avg_del_ind', 'da_avg_del_ind', 'carrier_avg_del_ind', 'poten_for_del', 'prev_fl_del']\n  else:\n    raise Exception(\"Invalid hypothesis number!\")\n\n  # drop any columns that are a no-no in the model and drop dep_is_delayed since it has to be numeric (int)\n  desired_categorical = [x for x in desired_categorical if x not in cols_to_drop + custom_cols_to_drop + ['dep_is_delayed']]\n\n  # confirm no duplicates\n  assert(len(desired_categorical) == len(set(desired_categorical)))\n\n  # confirm data actually has these features\n  # also confirm that the desired_categorical is part of the \"registered\" categorical features to choose from\n  all_cols = [t[0] for t in df.dtypes]\n  for dc in desired_categorical:\n    if dc not in all_cols:\n      raise Exception(\"Unknown feature found: {}\".format(dc))\n    if dc not in categorical_features:\n      raise Exception(\"Feature: {} is not a registered categorical feature\".format(dc))\n  \n  # ensure the vars are converted to strings\n  df = set_feature_dtype(df, desired_categorical, dtype='string')\n  \n  return df, list(set(desired_categorical))\n\ndef get_std_desired_numeric_int(df, desired_numeric):  \n  return [x for x in desired_numeric if get_feature_dtype(df, x) == 'int']\n\ndef get_std_desired_numeric_double(df, desired_numeric):\n  return [x for x in desired_numeric if get_feature_dtype(df, x) == 'double']\n\ndef get_std_to_convert_numeric(df, desired_numeric):\n  desired_numeric_int = get_std_desired_numeric_int(df, desired_numeric)\n  desired_numeric_double =  get_std_desired_numeric_double(df, desired_numeric)\n\n  to_convert_numeric = [x for x in desired_numeric if x not in desired_numeric_int + desired_numeric_double]\n  if verbose:\n    print(\"These columns need to be converted to numeric type: {}\".format(to_convert_numeric))\n    \n  return to_convert_numeric\n\ndef get_proportion_labels(df):\n  if verbose:\n    print(\"In method - get_proportion_labels - displaying proportion of labeled class\")\n    print(display(df.groupby('dep_is_delayed').count()))\n  \n  positive = df.filter(df.dep_is_delayed == 1).count()\n  negative = df.filter(df.dep_is_delayed == 0).count()\n  total = negative + positive\n  if total == 0:\n    raise Exception(\"No records found!\")\n  \n  if positive == 0:\n    raise Exception(\"No positive records found!\")\n  \n  if negative == 0:\n    raise Exception(\"No negative records found!\")\n    \n  # there is a risk that the positive/negative classes are so imbalanced that they are non existent in the df\n  # so we should guard against that case in order to avoid throwing div by 0\n  np = -1 if positive == 0 else 1.0 * negative/positive\n  pn = -1 if negative == 0 else 1.0 * positive/negative\n  \n  return 1.0 * positive/total, 1.0 * negative/total, pn, np\n\ndef downsample(df, min_major_class_ratio, alpha=0.99):\n  if min_major_class_ratio == -1:\n    # assign default value to reduce the majority class by half\n    min_major_class_ratio = 0.5\n    print(\"In method downsample: Warning - reset min_major_class_ratio to default: {}\".format(min_major_class_ratio))\n    \n  if verbose:\n    print(\"Starting to downsample, negative class has {} rows and positive class has {} rows\".format(df.filter(df.dep_is_delayed == 0).count(), df.filter(df.dep_is_delayed == 1).count()))\n    \n  negative = df.filter(df.dep_is_delayed == 0).sample(False, min_major_class_ratio * alpha, seed=2021)\n  positive = df.filter(df.dep_is_delayed == 1)\n  \n  new_df = positive.union(negative).cache()\n  if verbose:\n    negative = new_df.filter(new_df.dep_is_delayed ==0).count()\n    positive = new_df.filter(new_df.dep_is_delayed ==1).count()\n    print(\"After downsampling, negative class has {} rows and positive class has {} rows\".format(negative, positive))\n  \n  return new_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db94759a-dcbd-460b-8802-5d6c1f9586ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Logit Specific Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c202cb8e-fc49-4eac-9433-2b1793235563"}}},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql.types import StringType,BooleanType,DateType\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\nfrom pyspark.ml.feature import IndexToString, StringIndexer, OneHotEncoder, VectorAssembler, Bucketizer, StandardScaler\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\ndef get_train_test_finalset_for_logit_2(train, test, custom_payload, drop_na = True, set_handle_invalid=\"keep\"):\n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be null as it contains feature selection info\")\n  \n  categorical_features = custom_payload[\"categorical_features\"]\n  numeric_features = custom_payload[\"numeric_features\"]\n  \n  stages = []\n  for feature in categorical_features:\n    # string index categorical features:\n    indexer = StringIndexer(inputCol=feature, outputCol = feature+'_index')\n    indexer.setHandleInvalid(set_handle_invalid)\n    # one-hot the categorical features:\n    one_hot_encoder = OneHotEncoder(inputCols=[indexer.getOutputCol()], outputCols=[feature+'_Indicator'])\n    stages += [indexer, one_hot_encoder]\n  \n  # convert_label\n  label_stringIdx = StringIndexer(inputCol = 'dep_is_delayed', outputCol = 'label')\n  stages += [label_stringIdx]\n  \n  # convert numerical features\n  vector_assembler = VectorAssembler(inputCols = numeric_features, outputCol=\"numeric_vec\")\n  vector_assembler.setHandleInvalid(set_handle_invalid)\n  scaler = StandardScaler(inputCol=\"numeric_vec\", outputCol=\"scaled_features_1\")\n  \n  stages += [vector_assembler, scaler]\n  \n  # feature assembler\n  assemblerInputs = [feature + \"_Indicator\" for feature in categorical_features] + ['scaled_features_1']\n  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"scaled_features\")\n  stages += [assembler]\n  pipeline = Pipeline(stages = stages)\n  pipelineModel = pipeline.fit(train)\n  \n  # comb_features = categorical_features + numeric_features + ['dep_is_delayed']\n  \n  # transforming the data using pipeline\n  df_train = pipelineModel.transform(train)\n  selectedCols = ['label', 'scaled_features'] # + comb_features\n  df_train = df_train.select(selectedCols)\n  df_test = pipelineModel.transform(test)\n  df_test = df_test.select(selectedCols)\n  \n  if verbose:\n    print(\"Training Dataset Count Before Dropping NA: \" + str(df_train.count()))\n    print(\"Test Dataset Count Before Dropping NA: \" + str(df_test.count()))\n    # display(training_set)\n  \n  if drop_na:  \n    df_train = df_train.dropna()\n    df_test = df_test.dropna()\n  else:\n    print(\"Drop NA is set to false, will not drop any rows...\")\n  \n  if verbose and drop_na:\n    print(\"Training Dataset Count After Dropping NA: \" + str(df_train.count()))\n    print(\"Test Dataset Count After Dropping NA: \" + str(df_test.count()))\n    \n  # convert label to integer type, so we can compute performance metrics easily\n  df_train = df_train.withColumn('label', df_train['label'].cast(IntegerType()))  \n  df_test = df_test.withColumn('label', df_test['label'].cast(IntegerType()))\n  \n  return df_train, df_test\n\ndef get_train_test_finalset_for_logit(train, test, custom_payload, drop_na = True, set_handle_invalid=\"keep\"):\n  \n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be null as it contains feature selection info\")\n    \n  categorical_features = custom_payload[\"categorical_features\"]\n  numeric_features = custom_payload[\"numeric_features\"]\n  \n  # form a string indexer and change name of dep_is_delayed to \"label\" - used in std naming conventions in models  \n  # https://stackoverflow.com/questions/34681534/spark-ml-stringindexer-handling-unseen-labels\n  labelIndexer = StringIndexer(inputCol=\"dep_is_delayed\", outputCol=\"label\").setHandleInvalid(set_handle_invalid).fit(train)\n  train = labelIndexer.transform(train)\n  test = labelIndexer.transform(test)\n\n  # create index for each categorical feature\n  categorical_index = [i + \"_Index\" for i in categorical_features]\n  stringIndexer = StringIndexer(inputCols=categorical_features, outputCols=categorical_index).setHandleInvalid(set_handle_invalid).fit(train)\n  train = stringIndexer.transform(train)\n  test = stringIndexer.transform(test)\n\n  # create indicator feature for each categorical variable and do one hot encoding, encode only train data\n  list_encoders = [i + \"_Indicator\" for i in categorical_features]\n  encoder = OneHotEncoder(inputCols=categorical_index, outputCols=list_encoders).setHandleInvalid(set_handle_invalid).fit(train)\n  train_one_hot = encoder.transform(train)\n  test_one_hot = encoder.transform(test)\n\n  # retain only encoded categorical columns, numeric features and label \n  train_one_hot = train_one_hot.select([\"label\"] + categorical_index + list_encoders + numeric_features) \n  test_one_hot = test_one_hot.select([\"label\"] + categorical_index + list_encoders + numeric_features)\n\n  if verbose:\n    print(\"Training Dataset Count Before Dropping NA: \" + str(train_one_hot.count()))\n    print(\"Test Dataset Count Before Dropping NA: \" + str(test_one_hot.count()))\n    # display(training_set)\n  \n  if drop_na:  \n    training_set = train_one_hot.dropna()\n    test_set = test_one_hot.dropna()\n  else:\n    print(\"Drop NA is set to false, will not drop any rows...\")\n    training_set = train_one_hot\n    test_set = test_one_hot\n    \n  # convert label to integer type, so we can compute performance metrics easily\n  training_set = training_set.withColumn('label', training_set['label'].cast(IntegerType()))  \n  test_set = test_set.withColumn('label', test_set['label'].cast(IntegerType()))\n\n  if verbose and drop_na:\n    print(\"Training Dataset Count After Dropping NA: \" + str(training_set.count()))\n    print(\"Test Dataset Count After Dropping NA: \" + str(test_set.count()))\n    # display(training_set)\n  \n  return training_set, test_set\n\ndef get_logit_pipeline(training_set, set_handle_invalid=\"keep\", grid_search_mode=True):\n  \n  # get features only\n  features_only = training_set.columns\n  features_only.remove(\"label\")\n\n  # Combine training input columns into a single vector column, \"features\" is the default column name for sklearn/pyspark feature df\n  # so we preserve that default name\n  assembler = VectorAssembler(inputCols=features_only,outputCol=\"features\").setHandleInvalid(set_handle_invalid)\n\n  # Scale features so we can actually use them in logit\n  # StandardScaler standardizes features by removing the mean and scaling to unit variance.\n  standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n  \n  # use scaled features in logit, with output column as \"label\"\n  lr = LogisticRegression(featuresCol = 'scaled_features', labelCol = 'label', maxIter=10)\n\n  # for ML Lib pipeline, build a pipeline that will assemble the features into a single vector, perform scaling, and do optionally logit\n  if grid_search_mode:\n    pipeline = Pipeline(stages=[assembler, standardscaler, lr])\n  else:\n    pipeline = Pipeline(stages=[assembler, standardscaler])\n    \n  return lr, pipeline\n\n\ndef model_train_logit_grid_search(training_set, test_set, pipeline, lr, ts_split):\n  # grid search is broken - fails with the following error mode\n  # https://stackoverflow.com/questions/58827795/requirement-failed-nothing-has-been-added-to-this-summarizer\n  # this error mode seems specific to the data it is training on - which is non deterministic based on our train-test size\n  # so we don't want to take a dependency on this method\n  # moreover - its unclear whether the \"numFolds\" param should be 1 or > 1 \n  # if we make it > 1 then we don't preserve the ordering of the time series data, which is important\n  result = {}\n  \n  # form param grid for searching across multiple params to find best model\n  paramGrid = ParamGridBuilder() \\\n    .addGrid(lr.threshold, [0.01, 0.1, 0.2, 0.3]) \\\n    .addGrid(lr.maxIter, [2, 5, 10]) \\\n    .addGrid(lr.regParam, [0.1, 0.2]) \\\n    .build()\n  \n  # set up cross validator with the pipeline, choose num cross == 1\n  # TODO: clarify on what numFolds should be\n  crossval = CrossValidator(estimator = pipeline,\n                          estimatorParamMaps = paramGrid,\n                          evaluator = BinaryClassificationEvaluator(),\n                          numFolds = 1)\n  \n  # https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html#pyspark.ml.evaluation.BinaryClassificationEvaluator.metricName\n  # https://stats.stackexchange.com/questions/99916/interpretation-of-the-area-under-the-pr-curve\n  evaluator_aupr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n  evaluator_auroc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n  \n  # fit the model\n  cvModel = crossval.fit(training_set)\n  \n  # return best model from all our models we trained on\n  best_model = cvModel.bestModel\n  best_param_dic = get_best_param_dic_metrics(best_model, False)\n\n  # review performance on training data \n  train_model = cvModel.transform(training_set)\n  aupr = evaluator_aupr.evaluate(train_model)\n  auroc = evaluator_auroc.evaluate(train_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(train_model)\n  result[\"train\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, best_param_dic)\n  \n  # review performance on test data \n  test_model = cvModel.transform(test_set)\n  aupr = evaluator_aupr.evaluate(test_model)\n  auroc = evaluator_auroc.evaluate(test_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(test_model)\n  result[\"test\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, best_param_dic)\n  \n  return result\n\ndef model_train_logit(training_set, test_set, pipeline, lr, ts_split, custom_payload):\n  result = {}\n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be none as it contains hyper-param information\")\n  \n  if pipeline != None:\n    # regular logit path\n    pipelineModel = pipeline.fit(training_set)\n    df_train = pipelineModel.transform(training_set)\n    df_train = df_train.select(['label', 'scaled_features'])\n\n    pipelineModel = pipeline.fit(test_set)\n    df_test = pipelineModel.transform(test_set)\n    df_test = df_test.select(['label', 'scaled_features'])\n  else:\n    # we've already fit the pipeline (logit_alt)\n    df_train = training_set\n    df_test = test_set\n    \n  # hyper param setting\n  lr.threshold = custom_payload[\"threshold\"] if \"threshold\" in custom_payload.keys() else 0.5\n  lr.maxIter = custom_payload[\"maxIter\"] if \"maxIter\" in custom_payload.keys() else 10\n  lr.regParam = custom_payload[\"regParam\"] if \"regParam\" in custom_payload.keys() else 0.5\n  \n  print(\"Starting training of Logit model with parameters - threshold: {}, max iterations: {}, regParam: {}\"\\\n        .format(lr.threshold, lr.maxIter, lr.regParam))\n  \n  lrModel = lr.fit(df_train)\n  \n  # set up evaluators\n  evaluator_aupr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n  evaluator_auroc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n  \n  # review performance on training data \n  train_model = lrModel.transform(df_train)\n  aupr = evaluator_aupr.evaluate(train_model)\n  auroc = evaluator_auroc.evaluate(train_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(train_model)\n  result[\"train\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, lrModel.summary)\n  \n  # review performance on test data \n  test_model = lrModel.transform(df_test)\n  aupr = evaluator_aupr.evaluate(test_model)\n  auroc = evaluator_auroc.evaluate(test_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(test_model)\n  result[\"test\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, lrModel.summary)\n  \n  return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ccf7802-b33f-4d15-af54-04095fa8ef50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Random Forest Specific Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37665eb7-660a-4537-bcd0-f53de2b0ba0b"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, isnan, substring, split, when, lit, max as pyspark_max, countDistinct, count, mean, sum as pyspark_sum, expr, to_utc_timestamp, to_timestamp, concat, length\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import IntegerType, StringType, BooleanType, DateType, DoubleType\nimport pandas as pd\nfrom gcmap import GCMapper, Gradient\nimport matplotlib.pyplot as plt\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import IndexToString, StringIndexer, OneHotEncoder, VectorAssembler, Bucketizer, StandardScaler, VectorIndexer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.linalg import Vectors\nimport seaborn as sns\nfrom pyspark.ml.feature import QuantileDiscretizer\n  \ndef get_staged_data_for_trees(train, test, custom_payload):\n  stages = []\n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be none as it contains feature selection information\")\n  \n  categorical_features = custom_payload[\"categorical_features\"]\n  numeric_features = custom_payload[\"numeric_features\"] \n  num_buckets = custom_payload[\"num_buckets\"] if \"num_buckets\" in custom_payload.keys() else 3\n  quantize_numeric = custom_payload[\"quantize_numeric\"] if \"quantize_numeric\" in custom_payload.keys() else False\n  \n  for cat_feat in categorical_features:\n    # string indexing categorical features \n    stringIndexer = StringIndexer(inputCol=cat_feat, outputCol=cat_feat + \"_Index\").setHandleInvalid(\"keep\")\n    # one hot encode categorical features\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[cat_feat + \"_One_Hot\"])\n    # add to stages\n    stages += [stringIndexer, encoder]\n  \n  # create indexer for label class\n  labelIndexer = StringIndexer(inputCol=\"dep_is_delayed\", outputCol=\"label\").setHandleInvalid(\"keep\")\n  stages += [labelIndexer]\n  \n  print (\"Quantizing numeric features is set to {}. If set to true, we will use buckets = {}\".format(quantize_numeric, num_buckets))\n  if quantize_numeric:\n    for num_feat in numeric_features:\n      # bin numeric features \n      num_bin = QuantileDiscretizer(numBuckets=num_buckets, \n                                    inputCol=num_feat, outputCol=num_feat + \"_Binned\").setHandleInvalid(\"keep\")\n      stages += [num_bin]\n\n    # create vector assembler combining features into 1 vector (combining binner and categorical features)\n    assemblerInputs = [c + \"_One_Hot\" for c in categorical_features] + [n + \"_Binned\" for n in numeric_features]\n    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\").setHandleInvalid(\"keep\")\n    stages += [assembler]\n  else:\n    num_assembler = VectorAssembler(inputCols=numeric_features, outputCol=\"numeric_vec\").setHandleInvalid(\"skip\")\n    stages += [num_assembler]\n    assemblerInputs = [f + \"_One_Hot\" for f in categorical_features] + [\"numeric_vec\"]\n    # create vector assembler combining categorical and numeric_vec\n    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n    stages += [assembler]\n    \n  # notice no need for scaling in the case of RFs\n  # ensure that the model train eval functions for trees have feature column called \"features\" and not \"scaled_features\"\n  # it must match the output column from the processing phase here\n  pipeline = Pipeline().setStages(stages)\n  pipelineModel = pipeline.fit(train)\n  df_train = pipelineModel.transform(train)\n  \n  # features_comb = categorical_features + numeric_features + [\"dep_is_delayed\"]\n  selectedcols = [\"label\", \"features\"] # + features_comb\n  df_train = df_train.select(selectedcols)\n  df_test = pipelineModel.transform(test)\n  df_test = df_test.select(selectedcols)\n  \n  return df_train, df_test\n\ndef model_train_rf(train, test, ts_split, custom_payload):\n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be none as it contains hyper-param information\")\n    \n  result = {}\n  # convert label to integer type, so we can find performance metrics easily\n  train = train.withColumn('label', train['label'].cast(IntegerType()))  \n  test = test.withColumn('label', test['label'].cast(IntegerType()))\n  \n  # create an initial RandomForest model\n  rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n  \n  # hyper param setting\n  rf.maxBins = custom_payload[\"maxBins\"] if \"maxBins\" in custom_payload.keys() else 32\n  rf.numTrees = custom_payload[\"numTrees\"] if \"numTrees\" in custom_payload.keys() else 20\n  rf.minInstancesPerNode = custom_payload[\"minInstancesPerNode\"] if \"minInstancesPerNode\" in custom_payload.keys() else 10\n  rf.minInfoGain = custom_payload[\"minInfoGain\"] if \"minInfoGain\" in custom_payload.keys() else 0.001\n  print(\"Starting training of random forest model with parameters - max bins: {}, num trees: {}, minInstancesPerNode: {}, minInfoGain: {}\"\\\n        .format(rf.maxBins, rf.numTrees, rf.minInstancesPerNode, rf.minInfoGain))\n  \n  # train model with training data\n  rfModel = rf.fit(train)\n\n  # make predictions on test data \n  rf_predictions = rfModel.transform(test)\n  \n  # set up evaluators\n  evaluator_aupr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n  evaluator_auroc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n  \n  # review performance on training data \n  train_model = rfModel.transform(train)\n  aupr = evaluator_aupr.evaluate(train_model)\n  auroc = evaluator_auroc.evaluate(train_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(train_model)\n  result[\"train\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, rfModel.summary)\n  \n  # review performance on test data \n  test_model = rfModel.transform(test)\n  aupr = evaluator_aupr.evaluate(test_model)\n  auroc = evaluator_auroc.evaluate(test_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(test_model)\n  result[\"test\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, rfModel.summary)\n  \n  return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66d06136-8433-46d1-be08-945f110c88c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Gradient Boosted Trees Specific Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b4bed67-958d-48f0-b85f-d6abebba6da8"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\ndef model_train_gbt(train, test, ts_split, custom_payload):\n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be none as it contains hyper-param information\")\n    \n  result = {}\n  # convert label to integer type, so we can find performance metrics easily\n  train = train.withColumn('label', train['label'].cast(IntegerType()))  \n  test = test.withColumn('label', test['label'].cast(IntegerType()))\n  \n  # create an initial GBT model\n  gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n  \n  # hyper param setting\n  gbt.maxBins = custom_payload[\"maxBins\"] if \"maxBins\" in custom_payload.keys() else 32\n  gbt.maxDepth = custom_payload[\"maxDepth\"] if \"maxDepth\" in custom_payload.keys() else 10\n  gbt.minInstancesPerNode = custom_payload[\"minInstancesPerNode\"] if \"minInstancesPerNode\" in custom_payload.keys() else 10\n  gbt.minInfoGain = custom_payload[\"minInfoGain\"] if \"minInfoGain\" in custom_payload.keys() else 0.001\n  gbt.maxIter = custom_payload[\"maxIter\"] if \"maxIter\" in custom_payload.keys() else 10\n  gbt.stepSize = custom_payload[\"stepSize\"] if \"stepSize\" in custom_payload.keys() else 0.2\n  print(\"Starting training of GBT model with parameters - max bins: {}, max depth: {}, minInstancesPerNode: {}, minInfoGain: {}, max iterations: {}, step size: {}\"\\\n        .format(gbt.maxBins, gbt.maxDepth, gbt.minInstancesPerNode, gbt.minInfoGain, gbt.maxIter, gbt.stepSize))\n  \n  # train model with training data\n  gbtModel = gbt.fit(train)\n\n  # make predictions on test data\n  gbt_predictions = gbtModel.transform(test)\n  \n  # set up evaluators\n  evaluator_aupr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n  evaluator_auroc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n  \n  # review performance on training data \n  train_model = gbtModel.transform(train)\n  aupr = evaluator_aupr.evaluate(train_model)\n  auroc = evaluator_auroc.evaluate(train_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(train_model)\n  result[\"train\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, gbtModel) # no summary object exists in GBT\n  \n  # review performance on test data \n  test_model = gbtModel.transform(test)\n  aupr = evaluator_aupr.evaluate(test_model)\n  auroc = evaluator_auroc.evaluate(test_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(test_model)\n  result[\"test\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, gbtModel) # no summary object exists in GBT\n  \n  return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"238f4cb4-a8df-49c8-b860-3ada4ff51fe8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### SVM Specific Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c74188ae-fbf3-4398-8c78-5db4fc69a890"}}},{"cell_type":"code","source":["from pyspark.ml.classification import LinearSVC\n\ndef model_train_svm(training_set, test_set, pipeline, ts_split, custom_payload):\n  result = {}\n  if custom_payload == None:\n    raise Exception(\"Custom payload cannot be none as it contains hyper-param information\")\n  \n  if pipeline != None:\n    # regular svm implementation\n    pipelineModel = pipeline.fit(training_set)\n    df_train = pipelineModel.transform(training_set)\n    df_train = df_train.select(['label', 'scaled_features'])\n\n    pipelineModel = pipeline.fit(test_set)\n    df_test = pipelineModel.transform(test_set)\n    df_test = df_test.select(['label', 'scaled_features'])\n  else:\n    # svm_alt implementation\n    df_train = training_set\n    df_test = test_set\n  \n  svc = LinearSVC(featuresCol='scaled_features')\n  \n  # hyper param setting\n  svc.maxIter = custom_payload[\"maxIter\"] if \"maxIter\" in custom_payload.keys() else 40\n  svc.regParam = custom_payload[\"regParam\"] if \"regParam\" in custom_payload.keys() else 0.2\n  svc.aggregationDepth = custom_payload[\"aggregationDepth\"] if \"aggregationDepth\" in custom_payload.keys() else 2\n  svc.tol = custom_payload[\"tol\"] if \"tol\" in custom_payload.keys() else 1e-05\n  svc.threshold = custom_payload[\"threshold\"] if \"threshold\" in custom_payload.keys() else 0.0001\n  \n  print(\"Starting training of SVM model with parameters - aggregationDepth: {}, max iterations: {}, L2regParam: {}, convergence tolerance: {}, threshold: {}\"\\\n        .format(svc.aggregationDepth, svc.maxIter, svc.regParam, svc.tol, svc.threshold))\n  \n  svcModel = svc.fit(df_train)\n  \n  # set up evaluators\n  evaluator_aupr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n  evaluator_auroc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n  \n  # review performance on training data \n  train_model = svcModel.transform(df_train)\n  aupr = evaluator_aupr.evaluate(train_model)\n  auroc = evaluator_auroc.evaluate(train_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(train_model)\n  result[\"train\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, svcModel.extractParamMap())\n  \n  # review performance on test data \n  test_model = svcModel.transform(df_test)\n  aupr = evaluator_aupr.evaluate(test_model)\n  auroc = evaluator_auroc.evaluate(test_model)\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score = compute_classification_metrics(test_model)\n  result[\"test\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, svcModel.extractParamMap())\n  \n  return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0daf6f1d-a358-4de6-99b6-dfcb451465eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Model Training and Evaluation - Apply Pipeline To Data & Train & Collect Metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc210fb1-01c4-468f-8da9-8eb13184205e"}}},{"cell_type":"code","source":["from statistics import *\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n\ndef compute_classification_metrics(df):\n  # assumes df has columns called label and prediction\n  true_positive = df[(df.label == 1) & (df.prediction == 1)].count()\n  true_negative = df[(df.label == 0) & (df.prediction == 0)].count()\n  false_positive = df[(df.label == 0) & (df.prediction == 1)].count()\n  false_negative = df[(df.label == 1) & (df.prediction == 0)].count()\n  accuracy = ((true_positive + true_negative)/df.count())\n  \n  if (true_positive + false_negative == 0.0):\n    recall = 0.0\n    precision = float(true_positive) / (true_positive + false_positive)\n    \n  elif (true_positive + false_positive == 0.0):\n    recall = float(true_positive) / (true_positive + false_negative)\n    precision = 0.0\n    \n  else:\n    recall = float(true_positive) / (true_positive + false_negative)\n    precision = float(true_positive) / (true_positive + false_positive)\n\n  if(precision + recall == 0):\n    f1_score = 0\n    \n  else:\n    f1_score = 2 * ((precision * recall)/(precision + recall))\n    \n  return true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score\n\ndef get_classification_metrics(dic, with_display=True, display_train_metrics=False):\n  '''\n  assumes every model follows a contract of having a result dictionary\n  with key = \"train\" and key = \"test\", and optionally, key = \"val\"\n  \n  also assumes that the dictionary payload follows the format\n  result[\"key\"] = (true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, modelsummary)\n  '''\n  if \"train\" not in dic.keys() or \"test\" not in dic.keys():\n    raise Exception(\"Result object does not have the right keys\")\n  \n  contains_val = \"val\" in dic.keys()\n  result = {\"train\": dict(), \"test\": dict()}\n  if contains_val:\n    result[\"val\"] = dict()\n  \n  if not with_display:\n    display_train_metrics = False\n  \n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, modelsummary = dic[\"train\"]\n  # TODO: format this to human readable form\n  ts_split_str = str(ts_split)\n  \n  tmp = {\"true_positive\": true_positive, \"true_negative\": true_negative, \"false_positive\": false_positive, \"false_negative\": false_negative,\n         \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score, \"aupr\": aupr, \"auroc\": auroc, \"ts_split\": ts_split_str, \"summary\": modelsummary}\n  \n  if with_display:\n    # enter new line for neatness\n    print()\n    \n  # set the temp dictionary to result\n  result[\"train\"] = tmp\n  \n  str_ts = \"Metrics for Split - (Train: {}-{}), (Test: {}-{}), (Val: {}-{})\".format(ts_split[0].strftime(\"%b %d %Y\"), \\\n  ts_split[1].strftime(\"%b %d %Y\"), ts_split[2].strftime(\"%b %d %Y\"), ts_split[3].strftime(\"%b %d %Y\"), ts_split[4].strftime(\"%b %d %Y\"), ts_split[5].strftime(\"%b %d %Y\"))\n  \n  num = 150\n  \n  if with_display and display_train_metrics:\n    print(\"#\" * num)\n    print(\"Training Data \" + str_ts)\n    print(\"Accuracy: {}\".format(result[\"train\"][\"accuracy\"]))\n    print(\"Precision: {}\".format(result[\"train\"][\"precision\"]))\n    print(\"Recall: {}\".format(result[\"train\"][\"recall\"]))\n    print(\"F1 Score: {}\".format(result[\"train\"][\"f1_score\"]))\n    print(\"Area under PR curve: {}\".format(result[\"train\"][\"aupr\"]))\n    print(\"Area under ROC curve: {}\".format(result[\"train\"][\"auroc\"]))\n    print(\"#\" * num)\n  \n  # do the same for test and val\n  true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, modelsummary = dic[\"test\"]\n  # TODO: format this to human readable form\n  ts_split_str = str(ts_split)\n  \n  tmp = {\"true_positive\": true_positive, \"true_negative\": true_negative, \"false_positive\": false_positive, \"false_negative\": false_negative,\n         \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score, \"aupr\": aupr, \"auroc\": auroc, \"ts_split\": ts_split_str, \"summary\": modelsummary}\n  \n  # set the temp dictionary to result\n  result[\"test\"] = tmp\n  \n  if with_display:\n    print(\"#\" * num)\n    print(\"Test Data \" + str_ts)\n    print(\"Accuracy: {}\".format(result[\"test\"][\"accuracy\"]))\n    print(\"Precision: {}\".format(result[\"test\"][\"precision\"]))\n    print(\"Recall: {}\".format(result[\"test\"][\"recall\"]))\n    print(\"F1 Score: {}\".format(result[\"test\"][\"f1_score\"]))\n    print(\"Area under PR curve: {}\".format(result[\"test\"][\"aupr\"]))\n    print(\"Area under ROC curve: {}\".format(result[\"test\"][\"auroc\"]))\n    print(\"#\" * num)\n    \n  if contains_val:\n    true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1_score, aupr, auroc, ts_split, modelsummary = dic[\"val\"]\n    # TODO: format this to human readable form\n    ts_split_str = str(ts_split)\n    \n    tmp = {\"true_positive\": true_positive, \"true_negative\": true_negative, \"false_positive\": false_positive, \"false_negative\": false_negative,\n         \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score, \"aupr\": aupr, \"auroc\": auroc, \"ts_split\": ts_split_str, \"summary\": modelsummary}\n    \n    # set the temp dictionary to result\n    result[\"val\"] = tmp\n  \n    if with_display:\n      print(\"#\" * num)\n      print(\"Validation Data \" + str_ts)\n      print(\"Accuracy: {}\".format(result[\"val\"][\"accuracy\"]))\n      print(\"Precision: {}\".format(result[\"val\"][\"precision\"]))\n      print(\"Recall: {}\".format(result[\"val\"][\"recall\"]))\n      print(\"F1 Score: {}\".format(result[\"val\"][\"f1_score\"]))\n      print(\"Area under PR curve: {}\".format(result[\"val\"][\"aupr\"]))\n      print(\"Area under ROC curve: {}\".format(result[\"val\"][\"auroc\"]))\n      print(\"#\" * num)\n  \n  return result\n\n\ndef get_classification_metrics_for_storage_ingestion(list_dic):\n  # sadly, we cannot store model summary into the dataframe, thus we return everything except that\n  # assumes we have output from get_classification_metrics() (in list form) as the input here\n  for dic in list_dic:\n    dic[\"train\"].pop(\"summary\", None)\n    dic[\"test\"].pop(\"summary\", None)\n    if \"val\" in dic.keys():\n      dic[\"val\"].pop(\"summary\", None)\n  \n  return list_dic\n\ndef get_aggregated_classification_metrcs(list_dic, dtype=\"test\", with_display=True):\n  '''\n  gets summary stats (avg, min, percentiles etc.) for the list of models \n  has a dependency on the key naming defined in get_classification_metrics()\n  '''\n  metric_type = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"aupr\", \"auroc\"]\n  summary_type = [\"mean\", \"min\", \"max\", \"median\"]\n  \n  # for some reason math.min, math.max don't work and throw a \"type\" error\n  # same goes for statistics.mean and statistics.median\n  # this shows up sometimes in the RF models get_aggregated_classification_metrcs() calc\n  # so we go old school for now :)\n  def get_max(list_nums):\n    maxi = -1\n    for n in list_nums:\n      if n > maxi:\n        maxi = n\n    return float(maxi)\n  \n  def get_min(list_nums):\n    mini = 1000\n    for n in list_nums:\n      if n < mini:\n        mini = n\n    return float(mini)\n  \n  def get_mean(list_nums):\n    meanval = 0\n    for n in list_nums:\n      meanval += n\n    return 1.0 * meanval/len(list_nums)\n  \n  def get_median(list_nums):\n    list_nums.sort()\n    n = len(list_nums)\n    if n % 2 == 0:\n      median1 = list_nums[n//2]\n      median2 = list_nums[n//2 - 1]\n      median = (median1 + median2)/2\n    else:\n      median = list_nums[n//2]\n      \n    return float(median)\n    \n  todf = []\n  for s in summary_type:\n    metrics = []\n    for m in metric_type:\n      if s == \"mean\":\n        # print(\"For summary type: {} and metric type: {}, value is {}\".format(s, m, get_mean([dic[dtype][m] for dic in list_dic])))\n        metrics.append(get_mean([dic[dtype][m] for dic in list_dic]))\n      elif s == \"min\":\n        # print(\"For summary type: {} and metric type: {}, value is {}\".format(s, m, get_min([dic[dtype][m] for dic in list_dic])))\n        metrics.append(get_min([dic[dtype][m] for dic in list_dic]))\n      elif s == \"max\":        \n        # print(\"For summary type: {} and metric type: {}, value is {}\".format(s, m, get_max([dic[dtype][m] for dic in list_dic])))\n        metrics.append(get_max([dic[dtype][m] for dic in list_dic]))\n      elif s == \"median\":\n        # print(\"For summary type: {} and metric type: {}, value is {}\".format(s, m, get_median([dic[dtype][m] for dic in list_dic])))\n        metrics.append(get_median([dic[dtype][m] for dic in list_dic]))\n        \n    todf.append(tuple(metrics))   \n  \n  schema = StructType([ \\\n    StructField(\"accuracy\", DoubleType(), True), \\\n    StructField(\"precision\", DoubleType(), True), \\\n    StructField(\"recall\", DoubleType(), True), \\\n    StructField(\"f1_score\", DoubleType(), True), \\\n    StructField(\"AUPR\", DoubleType(), True), \\\n    StructField(\"AUROC\", DoubleType(), True) \\\n  ])\n  \n  df = spark.createDataFrame(data = todf, schema = schema)\n  if with_display:\n    print(\"Displaying aggregated metrics - rows are in order: {}\".format(summary_type))\n    display(df)\n    \n  return df\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d59566c6-a91f-41dc-bc71-0481f6c491a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# for each train, test split - apply pipeline and perform model training\n##### THIS IS THE MAIN METHOD FOR TRAINING AND PLUGGING IN ALL OTHER MODELS #####\n\ndef model_train_and_eval(data, splits, max_iter=1, model=\"logit\", collect_metrics = True, rebalance_downsample=True, rebalance_downsample_test=False, custom_payload=None):\n  '''\n  Main method for running models and returning results\n  custom_payload is referring to a dictionary that each model can unpack to access model specific values (reg params, special columns, feature slection etc.) \n  '''\n  \n  # list that holds the metrics results for the specified model\n  # the metrics returned per model may be different, \n  # look into each model's specific return format to extract relevant metric\n  collect_metrics_result = []\n  \n  for i in range(len(splits)):\n    if i > max_iter-1:\n      break\n    \n    train = get_df_for_model(data, splits, index=i, datatype=\"train\")\n    test = get_df_for_model(data, splits, index=i, datatype=\"test\")\n    #val = get_df_for_model(data, splits, index=i, datatype=\"val\")\n    \n    # drop planned_departure_utc and index id before sending off to the model\n    # we kept planned_departure_utc up till now as that's needed for data time filtering\n    # we kept index_id because its the index and may help spark in retrieving rows quicker\n    cols = [t[0] for t in train.dtypes if t[0] != 'planned_departure_utc' or t[0] != 'index_id']\n    train = train.select(cols).cache()\n    test = test.select(cols).cache()\n    \n    # need to pass down split dates info to the models as they need this for result object\n    split = get_dates_from_splits(splits, index=i, dtype=\"all\")\n    \n    # finally, downsample the majority class (dep_is_delayed == false) if need be\n    # we downsample because we have tons of data fortunately - otherwise, we would have up sampled\n    if rebalance_downsample:\n      print(\"Down-sampling the training data to have more balanced classes...\")\n      pt, nt, pn, np = get_proportion_labels(train)\n      train = downsample(train, pn)\n      if rebalance_downsample_test:\n        print(\"Down-sampling the test data to predict on balanced class scenario during prediction time...\")\n        pt, nt, pn, np = get_proportion_labels(test)\n        test = downsample(test, pn)\n      \n    print(\"Starting training iteration: {} for model: '{}' with collect_metrics: {}\".format(i+1, model, collect_metrics))\n    \n    if model == \"logit\":\n      training_set, test_set = get_train_test_finalset_for_logit(train, test, custom_payload)\n      lr, pipeline = get_logit_pipeline(training_set, grid_search_mode=False)\n      result = model_train_logit(training_set, test_set, pipeline, lr, split, custom_payload)\n    \n    # a different function for processing the data is applied in this mode\n    elif model == \"logit_alt\":\n      training_set, test_set = get_train_test_finalset_for_logit_2(train, test, custom_payload)\n      lr = LogisticRegression(featuresCol = 'scaled_features', labelCol = 'label')\n      result = model_train_logit(training_set, test_set, None, lr, split, custom_payload)\n      \n    # do not use gs version of logit - there is a bug - see func defn\n    elif model == \"logit_gs\":\n      training_set, test_set = get_train_test_finalset_for_logit(train, test, custom_payload)\n      lr, pipeline = get_logit_pipeline(training_set, grid_search_mode=True)\n      result = model_train_logit_grid_search(training_set, test_set, pipeline, lr, split)\n    \n    elif model == \"rf\":\n      train, test = get_staged_data_for_trees(train, test, custom_payload)\n      result = model_train_rf(train, test, split, custom_payload)\n    \n    elif model == \"gbt\":\n      train, test = get_staged_data_for_trees(train, test, custom_payload)\n      result = model_train_gbt(train, test, split, custom_payload)\n    \n    elif model == \"svm\":\n      # reuses logit pre-processing\n      training_set, test_set = get_train_test_finalset_for_logit(train, test, custom_payload)\n      lr, pipeline = get_logit_pipeline(training_set, grid_search_mode=False)\n      result = model_train_svm(training_set, test_set, pipeline, split, custom_payload)\n    \n    # a different function for processing the data is applied in this mode\n    elif model == \"svm_alt\":\n      # reuses logit pre-processing\n      training_set, test_set = get_train_test_finalset_for_logit_2(train, test, custom_payload)\n      result = model_train_svm(training_set, test_set, None, split, custom_payload)\n      \n    else:\n      raise Exception(\"Model name not found - given name is {}\".format(model))\n      \n    if collect_metrics:\n      collect_metrics_result.append(result)\n    \n  return collect_metrics_result\n      "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adeb16c2-9f4a-4224-8906-56f6c0eed90b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Driver Program - Logistic Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bb93366-bcce-4cf5-8c61-997ccf3193d4"}}},{"cell_type":"code","source":["###### WARNING: DO NOT MODIFY THE \"data\" OBJECT ######\n###### IT IS SHARED AMONGST OTHER DRIVER PROGRAMS ######\n\n# verbose logging / debug mode\n# can be changed per driver program\nverbose = True\n\nif verbose:\n  print(\"Total number of rows in original dataset are {}\".format(n))\n\n# all_cols, cols_to_consider, cols_to_drop, numeric_features, categorical_features, dt_features, bool_features = get_std_features(data)  \n\n\ndef get_values_from_hypothesis(hypothesis=1, custom_cols_to_drop=[]):\n  ##### HYPOTHESIS ######\n  data_, desired_numeric_h = get_std_desired_numeric(data, hypothesis= hypothesis, custom_cols_to_drop= custom_cols_to_drop)\n  data_, desired_categorical_h = get_std_desired_categorical(data_, hypothesis= hypothesis, custom_cols_to_drop= custom_cols_to_drop)\n\n  # assert label is numeric - this is because its needed for the classification metrics\n  assert(get_feature_dtype(data_, 'dep_is_delayed') == 'int')\n  cols_to_consider_h = list(set(desired_numeric_h + desired_categorical_h)) \n  \n  # we added dep_is_delayed to desired_numeric_h as we wanted to convert it to numeric\n  # however, it should not be part of the features list as it is the output var\n  # we will later add this col to cols_to_consider so its still part of our dataset\n  try:\n    desired_numeric_h.remove('dep_is_delayed')\n    desired_categorical_h.remove('dep_is_delayed')\n  except:\n    pass\n  \n  # ensure label and planned_departure_utc are present in cols_to_consider\n  cols_to_consider_h = add_required_cols(cols_to_consider_h)\n  # +2 in assert comes from adding planned_departure_utc and label (dep_is_delayed)\n  assert(len(cols_to_consider_h) == len(desired_numeric_h) + len(desired_categorical_h) + 2)\n  \n  # create custom payload object\n  custom_payload = {\"categorical_features\": desired_categorical_h, \"numeric_features\": desired_numeric_h}\n  \n  return desired_categorical_h, desired_numeric_h, cols_to_consider_h, data_.select(cols_to_consider_h).cache(), custom_payload\n\ndesired_categorical_logit, desired_numeric_logit, cols_to_consider_logit, data_logit, custom_payload_logit = get_values_from_hypothesis(1)\n\n\n#### COMMON ####  \nif verbose:\n  print(\"Finally, there are {} categorical features and {} numeric features\".format(len(desired_categorical_logit), len(desired_numeric_logit)))\n  print(\"data_logit has {} rows\".format(data_logit.count()))\n  display(data_logit)\n\n# get the data split for time series\nsplits = get_timeseries_train_test_splits(data_logit, train_test_ratio=3, test_months=2, start_year=2015, end_year=2019)\n# splits = get_timeseries_train_test_splits(data_logit)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5be363c-db26-4a9f-879a-db29013529aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Display Result and Write Models to Storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e94cdb3f-1676-47b9-8e2c-5c62a3a0e7cc"}}},{"cell_type":"code","source":["# perform actual training with logit model, get back list of dictionaries (each dic has train, test, val keys)\nlogit_results = model_train_and_eval(data_logit, splits, max_iter=4, model = \"logit\", collect_metrics = True, custom_payload = custom_payload_logit)\n\nstorage_logit_results = []\nfor lrdic in logit_results:\n  # get back well formed metrics dictionary for each time-iteration of the model\n  metrics = get_classification_metrics(lrdic, with_display=True, display_train_metrics=True)\n  storage_logit_results.append(metrics)\n  \nprint(\"Displaying Aggregated Results for Logistic Regression\")\nget_aggregated_classification_metrcs(storage_logit_results, dtype=\"test\", with_display=True)\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"385829a6-e909-4d38-bb1f-4d80eca49dde"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Writing results to storage\")\n# get back formatted dictionary list that is compatible to write to storage\nstorage_logit_results = get_classification_metrics_for_storage_ingestion(storage_logit_results)\nwrite_model_to_storage(storage_logit_results, \"logit_h1i4d_test\")\n\ndisplay(read_model_from_storage(\"logit_h1i4d_test\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"581a78ca-dcdf-423d-a9ff-dd98e8ac931d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Driver Program - Random Forests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fda38f0-6b6a-4af1-a0cc-5b788deef857"}}},{"cell_type":"code","source":["###### WARNING: DO NOT MODIFY THE \"data\" OBJECT ######\n###### IT IS SHARED AMONGST OTHER DRIVER PROGRAMS ######\n\n# verbose logging / debug mode\n# can be changed per driver program\nverbose = True\n\nif verbose:\n  print(\"Total number of rows in original dataset are {}\".format(n))\n\ndesired_categorical_rf, desired_numeric_rf, cols_to_consider_rf, data_rf, custom_payload_rf = get_values_from_hypothesis(1)\n\n#### COMMON ####  \nif verbose:\n  print(\"Finally, there are {} categorical features and {} numeric features\".format(len(desired_categorical_rf), len(desired_numeric_rf)))\n  print(\"data_rf has {} rows\".format(data_rf.count()))\n  display(data_rf)\n\n# get the data split for time series\nsplits = get_timeseries_train_test_splits(data_rf, train_test_ratio=3, test_months=2, start_year=2015, end_year=2019)\n# splits = get_timeseries_train_test_splits(data_rf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d30cae22-e3b5-4e2b-b774-dbd49860d7ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Display Result and Write Models to Storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4705a6f2-917a-4f2c-8c95-df9fa0fa5451"}}},{"cell_type":"code","source":["# perform actual training with RF model\nrf_results = model_train_and_eval(data_rf, splits, max_iter=4, model = \"rf\", collect_metrics = True, custom_payload = custom_payload_rf)\n\nstorage_rf_results = []\nfor rfdic in rf_results:\n  # get back well formed metrics dictionary for each time-iteration of the model\n  metrics = get_classification_metrics(rfdic, with_display=True, display_train_metrics=True)\n  storage_rf_results.append(metrics)  \n\nprint(\"Displaying Aggregated Results for Random Forests\")\nget_aggregated_classification_metrcs(storage_rf_results, dtype=\"test\", with_display=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2ed9f1b-772f-4b25-85c0-e116c2a6b4ac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Writing results to storage\")\n# get back formatted dictionary list that is compatible to write to storage\nstorage_rf_results = get_classification_metrics_for_storage_ingestion(storage_rf_results)\nwrite_model_to_storage(storage_rf_results, \"rf_h1i4d_test\")\n\ndisplay(read_model_from_storage(\"rf_h1i4d_test\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66d3da58-59d6-4f7c-a203-855e6bea9eef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"731952fb-fd9e-4808-abbf-e5dcac33705a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Driver Program - GBT"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85741a6c-7842-4be8-a301-735be426cfcd"}}},{"cell_type":"code","source":["###### WARNING: DO NOT MODIFY THE \"data\" OBJECT ######\n###### IT IS SHARED AMONGST OTHER DRIVER PROGRAMS ######\n\n# verbose logging / debug mode\n# can be changed per driver program\nverbose = True\n\nif verbose:\n  print(\"Total number of rows in original dataset are {}\".format(n))\n\ndesired_categorical_gbt, desired_numeric_gbt, cols_to_consider_gbt, data_gbt, custom_payload_gbt = get_values_from_hypothesis(1)\n    \n#### COMMON ####  \nif verbose:\n  print(\"Finally, there are {} categorical features and {} numeric features\".format(len(desired_categorical_gbt), len(desired_numeric_gbt)))\n  print(\"data_gbt has {} rows\".format(data_gbt.count()))\n  display(data_gbt)\n\n# get the data split for time series\nsplits = get_timeseries_train_test_splits(data_gbt, train_test_ratio=3, test_months=2, start_year=2015, end_year=2019)\n# splits = get_timeseries_train_test_splits(data_gbt)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24614bb4-f14c-42b8-9382-8dfe51583faf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Display Result and Write Models to Storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccc8d0cb-9c19-4358-94a9-98c502ab72ad"}}},{"cell_type":"code","source":["# perform actual training with GBT model\ngbt_results = model_train_and_eval(data_gbt, splits, max_iter=4, model = \"gbt\", collect_metrics = True, custom_payload = custom_payload_gbt)\n\nstorage_gbt_results = []\nfor gbtdic in gbt_results:\n  # get back well formed metrics dictionary for each time-iteration of the model\n  metrics = get_classification_metrics(gbtdic, with_display=True, display_train_metrics=True)\n  storage_gbt_results.append(metrics)  \n\nprint(\"Displaying Aggregated Results for GBT\")\nget_aggregated_classification_metrcs(storage_gbt_results, dtype=\"test\", with_display=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4249dd36-c7b2-4cd7-a480-c1fd3035d6d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Writing results to storage\")\n# get back formatted dictionary list that is compatible to write to storage\nstorage_gbt_results = get_classification_metrics_for_storage_ingestion(storage_gbt_results)\nwrite_model_to_storage(storage_gbt_results, \"gbt_h1i4d_test\")\n\ndisplay(read_model_from_storage(\"gbt_h1i4d_test\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"416308a9-6516-4153-832e-a0188f4c407f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Driver Program - SVM"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9154d644-f6ec-498e-91bc-cf6975d82e44"}}},{"cell_type":"code","source":["###### WARNING: DO NOT MODIFY THE \"data\" OBJECT ######\n###### IT IS SHARED AMONGST OTHER DRIVER PROGRAMS ######\n\n# verbose logging / debug mode\n# can be changed per driver program\nverbose = True\n\nif verbose:\n  print(\"Total number of rows in original data set are {}\".format(data.count()))\n\ndesired_categorical_svm, desired_numeric_svm, cols_to_consider_svm, data_svm, custom_payload_svm = get_values_from_hypothesis(1)\n\n#### COMMON ####  \nif verbose:\n  print(\"Finally, there are {} categorical features and {} numeric features\".format(len(desired_categorical_svm), len(desired_numeric_svm)))\n  print(\"data_gbt has {} rows\".format(data_svm.count()))\n  display(data_svm)\n\n# get the data split for time series\nsplits = get_timeseries_train_test_splits(data_svm, train_test_ratio=3, test_months=2, start_year=2015, end_year=2019)\n# splits = get_timeseries_train_test_splits(data_svm)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1767f85-2690-4b0d-9803-7c635669a382"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Display Result and Write Models to Storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66e1037c-b9e6-4753-b1e4-8bc1a3768e97"}}},{"cell_type":"code","source":["# perform actual training with SVM model\nsvm_results = model_train_and_eval(data_svm, splits, max_iter=4, model = \"svm\", collect_metrics = True, custom_payload = custom_payload_svm)\n\nstorage_svm_results = []\nfor svmdic in svm_results:\n  # get back well formed metrics dictionary for each time-iteration of the model\n  metrics = get_classification_metrics(svmdic, with_display=True, display_train_metrics=True)\n  storage_svm_results.append(metrics)  \n\nprint(\"Displaying Aggregated Results for SVM\")\nget_aggregated_classification_metrcs(storage_svm_results, dtype=\"test\", with_display=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91a0fc67-6838-431f-b6bb-4ecb95a9608c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Writing results to storage\")\n# get back formatted dictionary list that is compatible to write to storage\nstorage_svm_results = get_classification_metrics_for_storage_ingestion(storage_svm_results)\nwrite_model_to_storage(storage_svm_results, \"svm_h1i4d_test\")\n\ndisplay(read_model_from_storage(\"svm_h1i4d_test\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e188a17-d38d-4012-976e-f5306bacc83e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Resources and Links"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6758e8-22ca-4f2b-a8ac-2fd066710d69"}}},{"cell_type":"markdown","source":["- https://docs.databricks.com/applications/machine-learning/automl-hyperparam-tuning/mllib-mlflow-integration.html\n- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html\n- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\n- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n- https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n- https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n- https://medium.com/swlh/logistic-regression-with-pyspark-60295d41221\n- https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n- https://medium.com/@haoyunlai/smote-implementation-in-pyspark-76ec4ffa2f1d\n- https://docs.databricks.com/applications/machine-learning/train-model/mllib/index.html\n- https://github.com/MingChen0919/learning-apache-spark/blob/master/notebooks/06-machine-learning/classification/random-forest-classification.ipynb\n- https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07d9721a-3ac0-46c6-8d57-9135fe1d97a1"}}},{"cell_type":"code","source":["display(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ba3163e-5ceb-473b-88ec-d00f74941075"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["data.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7af55c95-76ae-4851-b856-7e297e1d1d5e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ec24aad-302f-4860-869a-7c5cc0b7f856"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Team07_Master_Models","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":418875703316836}},"nbformat":4,"nbformat_minor":0}
